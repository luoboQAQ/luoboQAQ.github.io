<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GNN on luoboQAQ</title><link>https://lbqaq.top/tags/gnn/</link><description>Recent content in GNN on luoboQAQ</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 21 Sep 2023 20:11:00 +0800</lastBuildDate><atom:link href="https://lbqaq.top/tags/gnn/index.xml" rel="self" type="application/rss+xml"/><item><title>Graph Unlearning</title><link>https://lbqaq.top/p/graph-unlearning/</link><pubDate>Wed, 13 Sep 2023 16:38:20 +0800</pubDate><guid>https://lbqaq.top/p/graph-unlearning/</guid><description>&lt;img src="https://lbqaq.top/p/graph-unlearning/110091745.webp" alt="Featured image of post Graph Unlearning" /&gt;&lt;h2 id="介绍"&gt;&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor"&gt;&lt;/a&gt;介绍
&lt;/h2&gt;&lt;p&gt;数据保护最近引起了越来越多的关注，并且已经提出了一些法规来保护个人用户的隐私。在这些法规中，提到了「被遗忘权」，它赋予数据主体从存储数据的实体中删除其数据的权利。这也意味着在机器学习中，模型提供者有义务消除其所有者要求被遗忘的数据的任何影响，即遗忘学习。&lt;/p&gt;
&lt;p&gt;最简单和有效的遗忘学习方法就是移除对应的样本后重新训练模型，然而当底层数据集很大时，这种方法在计算上可能令人望而却步。目前通用的遗忘学习方法是SISA (Sharded, Isolated, Sliced, and Aggregated) ——将训练集分为shards, shards中分为slices, 对于每个slice训练之后记录model parameters, 每个数据点被划分到不同的shards和slices中, unlearn时就是排除掉对应数据点然后retrain对应的shard和slices, 以空间开销换取训练的时间开销。&lt;/p&gt;
&lt;p&gt;对于图像和文本数据，分割数据没有什么问题。然而，对于图来说，GNN依赖于图结构信息，像在SISA中那样将节点随机划分为子图可能会严重损坏生成的模型。对此，作者提出了GraphEraser，以实现GNN中的遗忘学习。&lt;/p&gt;
&lt;p&gt;作者将图遗忘学习分为node unlearning「节点遗忘学习」和edge unlearning「边遗忘学习」，提出了两种图分割策略。第一种侧重于graph structural information「图结构信息」，另一种则是同时考虑graph structural and node feature information「图结构和节点特征信息」。&lt;/p&gt;
&lt;p&gt;为了同时考虑图结构和节点特征信息，作者将节点特征和图结构转化为嵌入向量，然后将其聚类为不同的shards。但是由于现实世界图的结构特性，传统的群落检测和聚类方法划分会导致分片大小不平衡，而大部分需要被撤销的数据都在最大的分区，从而导致效率低下。作者提出了两种分割算法和一种聚合算法以解决此问题。&lt;/p&gt;
&lt;h3 id="贡献"&gt;&lt;a href="#%e8%b4%a1%e7%8c%ae" class="header-anchor"&gt;&lt;/a&gt;贡献
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;第一次提出了在GNN模型上的遗忘学习方法&lt;/li&gt;
&lt;li&gt;提出了两种算法以平衡图分割块大小&lt;/li&gt;
&lt;li&gt;提出了一种基于学习的聚合方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="问题构造"&gt;&lt;a href="#%e9%97%ae%e9%a2%98%e6%9e%84%e9%80%a0" class="header-anchor"&gt;&lt;/a&gt;问题构造
&lt;/h2&gt;&lt;h3 id="问题定义"&gt;&lt;a href="#%e9%97%ae%e9%a2%98%e5%ae%9a%e4%b9%89" class="header-anchor"&gt;&lt;/a&gt;问题定义
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;节点遗忘学习&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于GNN模型$F_o$，每个数据主体的数据对应于GNN训练图$G_o$中的一个节点。数据主体$u$要删除其所有数据，则意味着从GNN的训练图中遗忘学习$u$的节点特征以及其于其他节点的链接。以社交网络为例，节点遗忘学习意味着需要从目标GNN的训练图中删除用户的个人资料信息和社交关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;边遗忘学习&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;数据主体$u$要删除其节点于另一个节点$v$之间的一条边缘。仍然以社交网络为例，边缘遗忘意味着社交网络用户想要隐藏他们与另一个人的关系。&lt;/p&gt;
&lt;h3 id="评估指标"&gt;&lt;a href="#%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87" class="header-anchor"&gt;&lt;/a&gt;评估指标
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;unlearning efficiency「遗忘学习效率」：与在训练的时间有关，时间要尽可能短。&lt;/li&gt;
&lt;li&gt;model utility「模型效用」：与准确性有关，越高越好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在之前提过，图分割存在分片大小不均匀的问题。为此，作者提出了两种分片目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G1: Balanced Shards「均衡分片」：每个分片中的节点数量相似。这样，每个分片的再训练时间是相似的，从而提高了整个图遗忘学习过程的效率。&lt;/li&gt;
&lt;li&gt;G2: Comparable Model Utility「可比模型效用」：图结构信息是决定GNN性能的主要因素，每个分片都应保留图的结构属性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="grapheraser框架构造"&gt;&lt;a href="#grapheraser%e6%a1%86%e6%9e%b6%e6%9e%84%e9%80%a0" class="header-anchor"&gt;&lt;/a&gt;GraphEraser框架构造
&lt;/h3&gt;&lt;p&gt;作者将GraphEraser框架分为三个阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Balanced Graph Partition「平衡图分区」：将训练图划分为不相交的分片&lt;/li&gt;
&lt;li&gt;Shard Model Training「分片训练模型」：对每个分片进行训练一个模型，称之为shard model「分片模型」$F_i$&lt;/li&gt;
&lt;li&gt;Shard Model Aggregation「分片模型聚合」：为了得到预测节点$w$的标签，将对应的数据（$w$的特征、其邻居的特征以及其中的图结构）同时发送到所有分片模型，并通过聚合所有分片模型的预测来获得最终预测。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GraphEraser框架的结构图如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230910155749458.png"
width="1609"
height="546"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230910155749458_hu_afc1e13c416b6b72.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230910155749458_hu_6fcb945e6404318f.png 1024w"
loading="lazy"
alt="GraphEraser框架图"
class="gallery-image"
data-flex-grow="294"
data-flex-basis="707px"
&gt;&lt;/p&gt;
&lt;h2 id="平衡图分割"&gt;&lt;a href="#%e5%b9%b3%e8%a1%a1%e5%9b%be%e5%88%86%e5%89%b2" class="header-anchor"&gt;&lt;/a&gt;平衡图分割
&lt;/h2&gt;&lt;p&gt;作者提出了三种图分区策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;策略0&lt;/strong&gt;：仅考虑节点特征信息，并随机对节点进行分区&lt;/p&gt;
&lt;p&gt;该策略可以满足G1「均衡分片」要求，但不满足G2「可比模型效用」要求&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;策略1&lt;/strong&gt;：依靠community detection「社区发现」，仅考虑结构信息，并尽可能保留它&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;策略2&lt;/strong&gt;：同时考虑结构信息和节点特征。将节点特征和图结构表示为低维向量，即节点嵌入，然后将节点嵌入聚类到不同的分片中。&lt;/p&gt;
&lt;p&gt;直接这样划分会导致划分区域不平衡，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911095231520.png"
width="970"
height="376"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911095231520_hu_6fc11e1147f5d99b.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911095231520_hu_d2a05acf1ae949d6.png 1024w"
loading="lazy"
alt="在Cora数据集上使用传统划分区域的大小"
class="gallery-image"
data-flex-grow="257"
data-flex-basis="619px"
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来，作者便介绍了对应的平衡图分区算法。&lt;/p&gt;
&lt;h3 id="社区发现算法"&gt;&lt;a href="#%e7%a4%be%e5%8c%ba%e5%8f%91%e7%8e%b0%e7%ae%97%e6%b3%95" class="header-anchor"&gt;&lt;/a&gt;社区发现算法
&lt;/h3&gt;&lt;p&gt;对于策略1，主要依赖的就是此算法。作者基于Label Propagation Algorithm (LPA)「标签传播算法」来设计图分区算法。在本文中，shard就是community。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;标签传播算法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911105605800.png"
width="898"
height="358"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911105605800_hu_1a8b28a41da05111.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911105605800_hu_adaf0536ba7d5e5.png 1024w"
loading="lazy"
alt="LPA工作流程"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="602px"
&gt;&lt;/p&gt;
&lt;p&gt;在初始阶段（图a），每个节点都随机分配一个分片标签。&lt;/p&gt;
&lt;p&gt;在标签传播阶段（图b → 图 c），每个节点都会发送自己的标签，将自己更新成收到最多的那个标签&lt;/p&gt;
&lt;p&gt;标签传播过程会对所有节点进行多次迭代，直到收敛（没有节点更改标签）&lt;/p&gt;
&lt;p&gt;就如之前提到的，传统的LPA会导致高度不平衡的图形分区，严重影响了遗忘学习的效率。&lt;/p&gt;
&lt;p&gt;对此，作者提出了一个实现平衡图分区的&lt;strong&gt;一般方法&lt;/strong&gt;。给定所需的分片大小$k$和最大分片大小$\delta$，为每个节点-分片定义一个可能被分配到此分片的preference「偏好值」，代表该节点被分配给了分片（这被称为destination shard「目标分片」），从而产生$k \times n$个偏好值。对这些值进行排序，如果目标分片中的节点数不超过$\delta$，就将该节点分配给此分片。&lt;/p&gt;
&lt;p&gt;具体而言，作者提出了Balanced LPA (BLPA)「平衡标签传播算法」，将偏好值定义为节点分片对的neighbor counts「邻居计数」（属于目标分片的邻居数量），并且具有较大邻居计数的节点分片对具有更高的优先级分配。&lt;/p&gt;
&lt;p&gt;算法的步骤如下：&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911142423655.png"
width="1005"
height="1271"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911142423655_hu_6cd374f3aae8569a.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911142423655_hu_92b8b6b78d535d99.png 1024w"
loading="lazy"
alt="BLPA算法"
class="gallery-image"
data-flex-grow="79"
data-flex-basis="189px"
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化：将每个节点随机分配给k个分片之一&lt;/li&gt;
&lt;li&gt;重新分配配置文件计算：对于每个节点$u$，使用元组$\left\langle u, \mathbb{C}_{s r c}, \mathbb{C}_{d s t}, \xi\right\rangle$表示其重新分配的配置文件，其中$\mathbb{C}_{s r c}$和$\mathbb{C}_{d s t}$是节点$u$的当前分片和目标分片，$\xi$是目标分片$\mathbb{C}_{d s t}$的邻居计数，并将其存入$\mathbb{F}$&lt;/li&gt;
&lt;li&gt;排序：邻居数量越多的重新分配配置文件应具有越高的优先级，所以按$\xi$对$\mathbb{F}$进行降序排序&lt;/li&gt;
&lt;li&gt;传播标签：枚举$\mathbb{F}$里的所有元素，如果$\mathbb{C}_{d s t}$的大小不超过给定的阈值$\delta$，就将其添加到目标分片并从当前分片中删除。之后在$\mathbb{F}$中删除所有剩余的包含节点$u$的元组。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;之后不断迭代，直到分片不更改或达到最大迭代$T$&lt;/p&gt;
&lt;p&gt;算法的时间复杂度为$O(n·d_{ave})$，$n$为节点数，$d_{ave}$为训练图的平均节点数。&lt;/p&gt;
&lt;p&gt;作者无法从理论上证明其收敛性，不过通过实验表示$T=30$时几乎是收敛的。&lt;/p&gt;
&lt;h3 id="嵌入式聚类算法"&gt;&lt;a href="#%e5%b5%8c%e5%85%a5%e5%bc%8f%e8%81%9a%e7%b1%bb%e7%ae%97%e6%b3%95" class="header-anchor"&gt;&lt;/a&gt;嵌入式聚类算法
&lt;/h3&gt;&lt;p&gt;对于策略2，作者使用预训练的GNN模型来获取所有节点嵌入，然后对生成的节点嵌入执行聚类。&lt;/p&gt;
&lt;p&gt;思路是将GNN模型的所有节点投影到空间中，再使用K-Means进行聚类。同样也会导致分块的不平均这个问题。&lt;/p&gt;
&lt;p&gt;对此，作者提出了Balanced Embedding k-means (BEKM)。定义preference「偏好值」为节点嵌入和所有节点分片对的分片质心之间的欧氏距离。&lt;/p&gt;
&lt;p&gt;具体的算法如下：&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912153952657.png"
width="998"
height="1447"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912153952657_hu_97849e8d384ef55b.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912153952657_hu_918b0ba534c5b6d2.png 1024w"
loading="lazy"
alt="BEKM算法"
class="gallery-image"
data-flex-grow="68"
data-flex-basis="165px"
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化：随机选择$k$个质心$C^0=\{C^0_1,C^0_2,\dots,C^0_k\}$&lt;/li&gt;
&lt;li&gt;计算嵌入质心距离：计算节点嵌入和质心之间的所有成对距离，从而得到$n\times k$个嵌入质心对。这些对存储在$\mathbb{F}$中。&lt;/li&gt;
&lt;li&gt;排序质心距离：距离较近的嵌入质心对具有更高的优先级，所以按照升序对$\mathbb{F}$进行排序&lt;/li&gt;
&lt;li&gt;重新分配节点和更新质心：枚举$\mathbb{F}$里的所有元素，如果$\mathbb{C}_{j}$的大小不超过给定的阈值$\delta$，就将其添加到目标分片。之后在$\mathbb{F}$中删除所有剩余的包含节点$i$的元组。最后，将新质心计算为其相应分片中所有节点的平均值。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;同样，不断重复直到分片不更改或达到最大迭代$T$&lt;/p&gt;
&lt;p&gt;算法的时间复杂度为$O(k·n)$，$n$个节点,$k$个分片。&lt;/p&gt;
&lt;h2 id="基于学习的聚合"&gt;&lt;a href="#%e5%9f%ba%e4%ba%8e%e5%ad%a6%e4%b9%a0%e7%9a%84%e8%81%9a%e5%90%88" class="header-anchor"&gt;&lt;/a&gt;基于学习的聚合
&lt;/h2&gt;&lt;p&gt;目前常见的聚合方式有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MajAggr：每个分片模型预测一个标签，取最多预测的标签&lt;/li&gt;
&lt;li&gt;MeanAggr：收集所有分片模型的后验向量，然后求平均值，得到聚合后验，选取最高后验值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者提出了一种基于学习的聚合方法LBAggr，为每个分片模型分配一个重要性分数，通过以下损失函数进行学习：&lt;/p&gt;
$$\min _{\alpha} \underset{w \in \mathcal{G}_{o}}{\mathbb{E}}\left[\mathcal{L}\left(\sum_{i=0}^{m} \alpha_{i} \cdot \mathcal{F}_{i}\left(X_{w}, \mathcal{N}_{w}\right), y\right)\right]+\lambda \sum_{i=0}^{m}\left\|\alpha_{i}\right\|$$&lt;p&gt;其中$X_{w}$和$\mathcal{N}_{w}$是训练图中节点$w$的特征向量和邻域，$y$是$w$的真实标签，$\mathcal{F}_{i}(\cdot)$表示分片模型$i$，$\alpha_{i}$是$\mathcal{F}_{i}(\cdot)$的重要性得分，$m$是分片总数。将所有重要性分数的总和调节为 1。&lt;/p&gt;
&lt;p&gt;作者通过梯度下降来找到最优的$\alpha$，从而解决最优化问题。然而，直接梯度下降会导致$\alpha$为负数。为了解决此问题，作者使用softmax 函数在每次迭代中进行归一化处理。&lt;/p&gt;
&lt;p&gt;为了提升运行速度，作者指出可以使用训练图中 10% 的节点进行重新训练。&lt;/p&gt;
&lt;h2 id="grapheraser"&gt;&lt;a href="#grapheraser" class="header-anchor"&gt;&lt;/a&gt;GraphEraser
&lt;/h2&gt;&lt;p&gt;将上面提到的方法集合在一起，就得到了此算法。当某些节点或边缘被数据所有者撤销时，只需要重新训练相应的分片模型即可。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912202314840.png"
width="1029"
height="1020"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912202314840_hu_72e08d63c3131ee4.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912202314840_hu_282650750a810a6b.png 1024w"
loading="lazy"
alt="GraphEraser"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="242px"
&gt;&lt;/p&gt;
&lt;h2 id="评估"&gt;&lt;a href="#%e8%af%84%e4%bc%b0" class="header-anchor"&gt;&lt;/a&gt;评估
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;数据集&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;作者采用了五个常用的图像数据集，分别为Cora, Citeseer, Pubmed, CS和Physics。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;作者在四个GNN模型上进行了测试，分别是SAGE，GCN，GAT和GIN。每个模型都经过100轮的训练，默认学习率设置为0.01，权重衰减为0.001。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;指标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在问题构造中提过，就是遗忘学习效率和模型效用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;遗忘学习效率：计算100个独立遗忘学习请求的平均遗忘学习时间。&lt;/li&gt;
&lt;li&gt;模型效用：使用F1得分&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;基线&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scratch「从头开始训练」&lt;/li&gt;
&lt;li&gt;Random「随机分区」&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实验设置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;将整个图分为两个不相交的部分，其中80%的节点用于训练GNN模型，20%的节点用于评估模型效用。&lt;/p&gt;
&lt;p&gt;分片大小$k$设为20, 20, 50, 30, 和 100&lt;/p&gt;
&lt;p&gt;片中节点最大个数$\delta$设为$\left \lceil \frac{n}{k} \right \rceil $&lt;/p&gt;
&lt;p&gt;最大迭代次数$T$设为30&lt;/p&gt;
&lt;p&gt;BLPA对应社区发现算法；BEKM对应嵌入式聚类算法&lt;/p&gt;
&lt;h3 id="遗忘学习效率评估"&gt;&lt;a href="#%e9%81%97%e5%bf%98%e5%ad%a6%e4%b9%a0%e6%95%88%e7%8e%87%e8%af%84%e4%bc%b0" class="header-anchor"&gt;&lt;/a&gt;遗忘学习效率评估
&lt;/h3&gt;&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921185539675.png"
width="1733"
height="464"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921185539675_hu_6f3a611eb1d0d762.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921185539675_hu_96450a638557c16b.png 1024w"
loading="lazy"
alt="遗忘学习效率"
class="gallery-image"
data-flex-grow="373"
data-flex-basis="896px"
&gt;&lt;/p&gt;
&lt;p&gt;如图所示，BLPA和BEKM相对于从头训练模型，可以显著的减少训练的时间。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921185950025.png"
width="991"
height="358"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921185950025_hu_cb8af87f07830d8c.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921185950025_hu_489c8bf421efc731.png 1024w"
loading="lazy"
alt="每个步骤所花费的时间"
class="gallery-image"
data-flex-grow="276"
data-flex-basis="664px"
&gt;&lt;/p&gt;
&lt;p&gt;相对于Random，时间略长是因为存在更长的图分割成本。BLPA 和 BEKM 都需要多次迭代以保留结构信息。但一旦完成图分割，就会将其固定下来。从这个意义上说，我们可以容忍这种代价，因为它只执行一次。&lt;/p&gt;
&lt;h3 id="模型效用评分"&gt;&lt;a href="#%e6%a8%a1%e5%9e%8b%e6%95%88%e7%94%a8%e8%af%84%e5%88%86" class="header-anchor"&gt;&lt;/a&gt;模型效用评分
&lt;/h3&gt;&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921193316971.png"
width="1960"
height="799"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921193316971_hu_3d1f0efb28430581.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921193316971_hu_e1ccddf03fd2d90f.png 1024w"
loading="lazy"
alt="不同遗忘学习方法模型效用"
class="gallery-image"
data-flex-grow="245"
data-flex-basis="588px"
&gt;&lt;/p&gt;
&lt;p&gt;绿色底色表示不需要聚合，红色底色表示作者提出的方法，蓝色字体表示最优得分。&lt;/p&gt;
&lt;p&gt;有些随机得分和BLPA和BEKM差不多，作者认为这时由于图结构信息在GNN模型中作用不大导致的。&lt;/p&gt;
&lt;p&gt;对此，作者提出了一个方法选择的技巧：可以首先比较MLP和GNN的F1分数，如果MLP和GNN之间的F1分数差距很小，随机方法可能是一个不错的选择，因为它更容易实现，并且可以实现与BLPA和BEKM相当的模型效用。否则，BLPA和BEKM是更好的选择，因为更好的模型实用程序。&lt;/p&gt;
&lt;p&gt;如果 GNN 遵循 GCN 结构，则可以选择BLPA，否则可以采用 BEKM。这是因为 GCN 模型需要节点度信息来进行归一化，而BLPA 可以保留更多的局部结构信息，从而更好地保留节点度。&lt;/p&gt;
&lt;p&gt;BEKM 在 Cora 数据集和 GIN 模型上的 F1 得分为 0.801，而 Scratch 的相应 F1 得分为 0.787。作者认为有两种可能的原因：抽样往往可以消除数据集中的一些“噪声”；其次，GraphEraser通过聚合所有子模型的结果来进行最终预测，从这个意义上说，GraphEraser执行集成，这是提高模型性能的另一种方法。&lt;/p&gt;
&lt;h3 id="lbaggr-的效果"&gt;&lt;a href="#lbaggr-%e7%9a%84%e6%95%88%e6%9e%9c" class="header-anchor"&gt;&lt;/a&gt;LBAggr 的效果
&lt;/h3&gt;&lt;p&gt;有效，可以提升F1分数。&lt;/p&gt;
&lt;p&gt;比较不同的GNN模型，GCN从LBAggr中受益最大，而GIN受益最少。在模型效用方面，GraphEraser-BLPA方法从LBAggr中受益最大。我们推测这是因为BLPA划分方法可以捕获局部结构信息，同时丢失训练图的一些全局结构信息。&lt;/p&gt;
&lt;p&gt;为了进一步提高忘却效率，可以使用训练图中的一小部分节点来学习重要性分数。这样做可以有效地减少 LBAggr 的遗忘学习时间。使用 10% 的节点和使用固定数量的 1，000 个节点都可以实现与使用所有节点相当的模型效用。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921194318907.png"
width="971"
height="561"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921194318907_hu_92b016a23587b81c.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921194318907_hu_71f9c13474b1d5.png 1024w"
loading="lazy"
alt="LBAggr 的效果"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
&gt;&lt;/p&gt;
&lt;h3 id="和其他遗忘学习方法对比"&gt;&lt;a href="#%e5%92%8c%e5%85%b6%e4%bb%96%e9%81%97%e5%bf%98%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95%e5%af%b9%e6%af%94" class="header-anchor"&gt;&lt;/a&gt;和其他遗忘学习方法对比
&lt;/h3&gt;&lt;p&gt;&lt;img src="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921200838804.png"
width="2011"
height="1003"
srcset="https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921200838804_hu_baf7ab0c311bd774.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230921200838804_hu_b15c3db71f522390.png 1024w"
loading="lazy"
alt="遗忘学习对比"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="481px"
&gt;&lt;/p&gt;
&lt;p&gt;同样，也是作者的方法比较好。&lt;/p&gt;</description></item></channel></rss>